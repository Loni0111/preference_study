{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install necessary modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pytorch package, this may take several minutes, please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/loni/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    pytorch-1.7.0              |          py3.7_0        63.4 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        63.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  libuv              pkgs/main/osx-64::libuv-1.40.0-haf1e3a3_0\n",
      "  typing_extensions  pkgs/main/noarch::typing_extensions-3.7.4.3-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  pytorch            pkgs/main::pytorch-1.6.0-cpu_py37hd70~ --> pytorch::pytorch-1.7.0-py3.7_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "pytorch-1.7.0        | 63.4 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c pytorch pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (6.2.0)\n",
      "Requirement already satisfied: torchvision in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (from torchvision) (1.17.2)\n",
      "Requirement already satisfied: torch==1.7.0 in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (from torchvision) (1.7.0)\n",
      "Requirement already satisfied: future in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (from torch==1.7.0->torchvision) (0.17.1)\n",
      "Requirement already satisfied: typing_extensions in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (from torch==1.7.0->torchvision) (0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Pillow torchvision matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (0.21.3)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.17.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/loni/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (0.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Image Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images in a dataset do not usually have the same pixel intensity and dimensions. In this section, you will pre-process the dataset by standardizing the pixel values. The next required process is transforming raw images into tensors so that the algorithm can process them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageDir</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>flowers/images/0.jpg</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flowers/images/1.jpg</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>flowers/images/10.jpg</td>\n",
       "      <td>0.431818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>flowers/images/100.jpg</td>\n",
       "      <td>0.477273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>flowers/images/101.jpg</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>paints/images/95.jpg</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1147</td>\n",
       "      <td>paints/images/96.jpg</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>paints/images/97.jpg</td>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1149</td>\n",
       "      <td>paints/images/98.jpg</td>\n",
       "      <td>0.613636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>paints/images/99.jpg</td>\n",
       "      <td>0.477273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1151 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    imageDir     score\n",
       "0       flowers/images/0.jpg  0.363636\n",
       "1       flowers/images/1.jpg  0.227273\n",
       "2      flowers/images/10.jpg  0.431818\n",
       "3     flowers/images/100.jpg  0.477273\n",
       "4     flowers/images/101.jpg  0.454545\n",
       "...                      ...       ...\n",
       "1146    paints/images/95.jpg  0.363636\n",
       "1147    paints/images/96.jpg  0.500000\n",
       "1148    paints/images/97.jpg  0.568182\n",
       "1149    paints/images/98.jpg  0.613636\n",
       "1150    paints/images/99.jpg  0.477273\n",
       "\n",
       "[1151 rows x 2 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data file was created in the data clearning part\n",
    "labels = pd.read_csv('score.csv')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 5 level of likeness\n",
    "def likeScore(score):\n",
    "    # very like\n",
    "    if score >= 0.8:\n",
    "        likeness = 4\n",
    "    # like\n",
    "    elif score >= 0.6:\n",
    "        likeness = 3\n",
    "    # midium\n",
    "    elif score >= 0.4:\n",
    "        likeness = 2\n",
    "    # dislike\n",
    "    elif score >= 0.2:\n",
    "        likeness = 1\n",
    "    # very dislike\n",
    "    else:\n",
    "        likeness = 0\n",
    "        \n",
    "    return likeness\n",
    "\n",
    "# replace score to likeness\n",
    "labels.score = labels.score.map(likeScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageDir</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>flowers/images/0.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flowers/images/1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>flowers/images/10.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>flowers/images/100.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>flowers/images/101.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>paints/images/95.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1147</td>\n",
       "      <td>paints/images/96.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>paints/images/97.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1149</td>\n",
       "      <td>paints/images/98.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>paints/images/99.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1151 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    imageDir  score\n",
       "0       flowers/images/0.jpg      1\n",
       "1       flowers/images/1.jpg      1\n",
       "2      flowers/images/10.jpg      2\n",
       "3     flowers/images/100.jpg      2\n",
       "4     flowers/images/101.jpg      2\n",
       "...                      ...    ...\n",
       "1146    paints/images/95.jpg      1\n",
       "1147    paints/images/96.jpg      2\n",
       "1148    paints/images/97.jpg      2\n",
       "1149    paints/images/98.jpg      3\n",
       "1150    paints/images/99.jpg      2\n",
       "\n",
       "[1151 rows x 2 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Resize images to 100 X 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (not apply the module in torchvision, because the original image sizes of three different categories are different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function create a folder for resized images.\n",
    "def imageResize(categoryList):\n",
    "    \n",
    "    for category in categoryList:\n",
    "        path = category + '/images/'\n",
    "    \n",
    "        # create a new folder for resized images\n",
    "        newpath = category + '/images_resized/'\n",
    "        os.makedirs(newpath)\n",
    "        dirs = os.listdir(path)\n",
    "\n",
    "        for item in dirs:\n",
    "            if os.path.isfile(path+item):\n",
    "                im = Image.open(path+item)\n",
    "                f, e = os.path.splitext(item)\n",
    "                imResize = im.resize((100,100), Image.ANTIALIAS)\n",
    "                imResize.save(newpath + '/' + f + '.jpg', 'JPEG', quality=90)\n",
    "                \n",
    "# Only execute the first time \n",
    "imageResize(['flowers', 'houses', 'paints'])\n",
    "\n",
    "# Update the image dir. in the dataframe, only\n",
    "labels['imageDir'] = labels['imageDir'].str.replace('images', 'images_resized')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageDir</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>flowers/images_resized/0.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flowers/images_resized/1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>flowers/images_resized/10.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>flowers/images_resized/100.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>flowers/images_resized/101.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>paints/images_resized/95.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1147</td>\n",
       "      <td>paints/images_resized/96.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>paints/images_resized/97.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1149</td>\n",
       "      <td>paints/images_resized/98.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>paints/images_resized/99.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1151 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            imageDir  score\n",
       "0       flowers/images_resized/0.jpg      1\n",
       "1       flowers/images_resized/1.jpg      1\n",
       "2      flowers/images_resized/10.jpg      2\n",
       "3     flowers/images_resized/100.jpg      2\n",
       "4     flowers/images_resized/101.jpg      2\n",
       "...                              ...    ...\n",
       "1146    paints/images_resized/95.jpg      1\n",
       "1147    paints/images_resized/96.jpg      2\n",
       "1148    paints/images_resized/97.jpg      2\n",
       "1149    paints/images_resized/98.jpg      3\n",
       "1150    paints/images_resized/99.jpg      2\n",
       "\n",
       "[1151 rows x 2 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the image dir. in the dataframe, only\n",
    "labels['imageDir'] = labels['imageDir'].str.replace('images', 'images_resized')\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Image standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before normalization, let's take a look at what the original tricolor matrix of an 100 x 100 image looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[30, 27, 21, ..., 16, 18, 21],\n",
       "        [27, 27, 24, ..., 18, 19, 21],\n",
       "        [25, 29, 29, ..., 20, 21, 22],\n",
       "        ...,\n",
       "        [18, 17, 21, ..., 19, 19, 21],\n",
       "        [17, 18, 22, ..., 19, 19, 21],\n",
       "        [15, 18, 21, ..., 19, 20, 21]],\n",
       "\n",
       "       [[33, 30, 22, ..., 30, 32, 35],\n",
       "        [30, 30, 25, ..., 32, 33, 35],\n",
       "        [28, 32, 30, ..., 34, 35, 36],\n",
       "        ...,\n",
       "        [21, 20, 24, ..., 33, 35, 37],\n",
       "        [20, 21, 25, ..., 35, 35, 37],\n",
       "        [18, 21, 24, ..., 35, 36, 40]],\n",
       "\n",
       "       [[26, 23, 16, ..., 13, 15, 18],\n",
       "        [23, 23, 19, ..., 15, 16, 18],\n",
       "        [21, 25, 24, ..., 17, 18, 19],\n",
       "        ...,\n",
       "        [10,  9, 13, ...,  7,  8, 10],\n",
       "        [ 9, 10, 14, ...,  8,  8, 10],\n",
       "        [ 7, 10, 13, ...,  8,  9, 12]]], dtype=uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input: image file\n",
    "# Output: 3 dimensional array (tricolour, pixel_rawIndex, pixel_colIndex)\n",
    "def jpg_2_arr(file):\n",
    "    img=Image.open(file)\n",
    "    r,g,b=img.split()\n",
    "\n",
    "    r_arr=np.array(r).reshape(100*100)\n",
    "    g_arr=np.array(g).reshape(100*100)\n",
    "    b_arr=np.array(b).reshape(100*100)\n",
    "\n",
    "    img_arr=np.concatenate((r_arr,g_arr,b_arr))\n",
    "    result=img_arr.reshape((3,100,100))\n",
    "    return result\n",
    "\n",
    "# Exp. image\n",
    "jpg_2_arr('flowers/images_resized/0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image normalization for better efficiency on the following propagation function \n",
    "# Declare image normalization for latter usage\n",
    "    \n",
    "# Here is ImageNet statistics\n",
    "means = [0.485, 0.456, 0.406] \n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(means,std)])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(means,std)])\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize(means,std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is an example of image matrix data after normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6042, -1.6555, -1.7583,  ..., -1.8439, -1.8097, -1.7583],\n",
       "         [-1.6555, -1.6555, -1.7069,  ..., -1.8097, -1.7925, -1.7583],\n",
       "         [-1.6898, -1.6213, -1.6213,  ..., -1.7754, -1.7583, -1.7412],\n",
       "         ...,\n",
       "         [-1.8097, -1.8268, -1.7583,  ..., -1.7925, -1.7925, -1.7583],\n",
       "         [-1.8268, -1.8097, -1.7412,  ..., -1.7925, -1.7925, -1.7583],\n",
       "         [-1.8610, -1.8097, -1.7583,  ..., -1.7925, -1.7754, -1.7583]],\n",
       "\n",
       "        [[-1.4580, -1.5105, -1.6506,  ..., -1.5105, -1.4755, -1.4230],\n",
       "         [-1.5105, -1.5105, -1.5980,  ..., -1.4755, -1.4580, -1.4230],\n",
       "         [-1.5455, -1.4755, -1.5105,  ..., -1.4405, -1.4230, -1.4055],\n",
       "         ...,\n",
       "         [-1.6681, -1.6856, -1.6155,  ..., -1.4580, -1.4230, -1.3880],\n",
       "         [-1.6856, -1.6681, -1.5980,  ..., -1.4230, -1.4230, -1.3880],\n",
       "         [-1.7206, -1.6681, -1.6155,  ..., -1.4230, -1.4055, -1.3354]],\n",
       "\n",
       "        [[-1.3513, -1.4036, -1.5256,  ..., -1.5779, -1.5430, -1.4907],\n",
       "         [-1.4036, -1.4036, -1.4733,  ..., -1.5430, -1.5256, -1.4907],\n",
       "         [-1.4384, -1.3687, -1.3861,  ..., -1.5081, -1.4907, -1.4733],\n",
       "         ...,\n",
       "         [-1.6302, -1.6476, -1.5779,  ..., -1.6824, -1.6650, -1.6302],\n",
       "         [-1.6476, -1.6302, -1.5604,  ..., -1.6650, -1.6650, -1.6302],\n",
       "         [-1.6824, -1.6302, -1.5779,  ..., -1.6650, -1.6476, -1.5953]]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Image.open('flowers/images_resized/0.jpg')\n",
    "train_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'train_test_split' package are from Sklearns, which is a popular tool in machine learning fields\n",
    "# Here the train-test data size ratio is 4:1 \n",
    "train_data, test_data = train_test_split(labels, test_size=0.2)\n",
    "\n",
    "# Validation dataset will be split from the traning dataset\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageDir</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>flowers/images_resized/162.jpg</td>\n",
       "      <td>0.659091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>flowers/images_resized/288.jpg</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>houses/images_resized/259.jpg</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>flowers/images_resized/375.jpg</td>\n",
       "      <td>0.386364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>flowers/images_resized/129.jpg</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>paints/images_resized/388.jpg</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>flowers/images_resized/289.jpg</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>paints/images_resized/210.jpg</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>flowers/images_resized/278.jpg</td>\n",
       "      <td>0.704545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>flowers/images_resized/348.jpg</td>\n",
       "      <td>0.613636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>828 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            imageDir     score\n",
       "71    flowers/images_resized/162.jpg  0.659091\n",
       "210   flowers/images_resized/288.jpg  0.727273\n",
       "564    houses/images_resized/259.jpg  0.295455\n",
       "307   flowers/images_resized/375.jpg  0.386364\n",
       "34    flowers/images_resized/129.jpg  0.227273\n",
       "...                              ...       ...\n",
       "1061   paints/images_resized/388.jpg  0.590909\n",
       "211   flowers/images_resized/289.jpg  0.500000\n",
       "865    paints/images_resized/210.jpg  0.363636\n",
       "199   flowers/images_resized/278.jpg  0.704545\n",
       "277   flowers/images_resized/348.jpg  0.613636\n",
       "\n",
       "[828 rows x 2 columns]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageDir</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>flowers/images_resized/38.jpg</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>flowers/images_resized/353.jpg</td>\n",
       "      <td>0.522727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>paints/images_resized/156.jpg</td>\n",
       "      <td>0.340909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>houses/images_resized/30.jpg</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>paints/images_resized/329.jpg</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>flowers/images_resized/359.jpg</td>\n",
       "      <td>0.613636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>paints/images_resized/110.jpg</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>paints/images_resized/360.jpg</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>paints/images_resized/333.jpg</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>paints/images_resized/275.jpg</td>\n",
       "      <td>0.613636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            imageDir     score\n",
       "312    flowers/images_resized/38.jpg  0.636364\n",
       "283   flowers/images_resized/353.jpg  0.522727\n",
       "804    paints/images_resized/156.jpg  0.340909\n",
       "608     houses/images_resized/30.jpg  0.590909\n",
       "996    paints/images_resized/329.jpg  0.681818\n",
       "...                              ...       ...\n",
       "289   flowers/images_resized/359.jpg  0.613636\n",
       "754    paints/images_resized/110.jpg  0.272727\n",
       "1031   paints/images_resized/360.jpg  0.363636\n",
       "1001   paints/images_resized/333.jpg  0.454545\n",
       "936    paints/images_resized/275.jpg  0.613636\n",
       "\n",
       "[231 rows x 2 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageDir</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>flowers/images_resized/63.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>flowers/images_resized/277.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1089</td>\n",
       "      <td>paints/images_resized/43.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>houses/images_resized/276.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>houses/images_resized/190.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>flowers/images_resized/282.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>houses/images_resized/241.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>houses/images_resized/253.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>houses/images_resized/228.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>paints/images_resized/253.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            imageDir  score\n",
       "360    flowers/images_resized/63.jpg      2\n",
       "198   flowers/images_resized/277.jpg      3\n",
       "1089    paints/images_resized/43.jpg      1\n",
       "582    houses/images_resized/276.jpg      3\n",
       "492    houses/images_resized/190.jpg      1\n",
       "...                              ...    ...\n",
       "204   flowers/images_resized/282.jpg      3\n",
       "546    houses/images_resized/241.jpg      1\n",
       "558    houses/images_resized/253.jpg      2\n",
       "531    houses/images_resized/228.jpg      3\n",
       "912    paints/images_resized/253.jpg      3\n",
       "\n",
       "[92 rows x 2 columns]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Pre-process data for loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a class for dataset format\n",
    "# input is the image score dataframe\n",
    "# output is the formatted image dataset and its score labels\n",
    "class formatDataset(Dataset):\n",
    "    def __init__(self, data, path , transform = None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path,label = self.data[index]\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place file in the working directory\n",
    "train_path = r'/train/'\n",
    "test_path = r'/test/'\n",
    "\n",
    "train_data = formatDataset(train_data.values, train_path, train_transform)\n",
    "valid_data = formatDataset(valid_data.values, train_path, valid_transform)\n",
    "test_data = formatDataset(test_data.values, test_path, test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_data, batch_size = 4, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(dataset = valid_data, batch_size = 4, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size = 4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use torch.device function to set GPU for parallel computation if there is an available GPU\n",
    "# If not, the program will use CPU by default\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Construct Convolution Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Set hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "num_classes = 2\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build and assemble forward propogation layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There will be 3 main layers in the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Convolution layer for feature abstraction\n",
    "#### 2) Pooling layer for removal of positional feature effects\n",
    "#### 3) Activation layer for non-linearity transformation\n",
    "#### Usually, there are more than one layers for each types of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Convolution neural Network for forward propagation\n",
    "class CNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(180, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 5))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 5))\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=180, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Loss function and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The most common cross-entropy loss is used in this project, which can be formulated as: \n",
    "#### logloss(N=1) = ylog(p) + (1 - y)log(1 - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the adaptive moment estimation (Adam) is applied as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device) # make sure the device option was run in previous step\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.981811 \tValidation Loss: 1.280269\n",
      "Epoch: 2 \tTraining Loss: 0.992623 \tValidation Loss: 1.273741\n",
      "Epoch: 3 \tTraining Loss: 0.959942 \tValidation Loss: 1.280848\n",
      "Epoch: 4 \tTraining Loss: 0.952684 \tValidation Loss: 1.281827\n",
      "Epoch: 5 \tTraining Loss: 0.967177 \tValidation Loss: 1.282847\n",
      "Epoch: 6 \tTraining Loss: 0.954350 \tValidation Loss: 1.290775\n",
      "Epoch: 7 \tTraining Loss: 0.946522 \tValidation Loss: 1.292814\n",
      "Epoch: 8 \tTraining Loss: 0.972833 \tValidation Loss: 1.293446\n",
      "Epoch: 9 \tTraining Loss: 0.944201 \tValidation Loss: 1.304687\n",
      "Epoch: 10 \tTraining Loss: 0.930092 \tValidation Loss: 1.312343\n",
      "Epoch: 11 \tTraining Loss: 0.940723 \tValidation Loss: 1.319807\n",
      "Epoch: 12 \tTraining Loss: 0.918080 \tValidation Loss: 1.311483\n",
      "Epoch: 13 \tTraining Loss: 0.907122 \tValidation Loss: 1.323493\n",
      "Epoch: 14 \tTraining Loss: 0.908888 \tValidation Loss: 1.324066\n",
      "Epoch: 15 \tTraining Loss: 0.927303 \tValidation Loss: 1.341262\n",
      "Epoch: 16 \tTraining Loss: 0.896654 \tValidation Loss: 1.337131\n",
      "Epoch: 17 \tTraining Loss: 0.908932 \tValidation Loss: 1.334482\n",
      "Epoch: 18 \tTraining Loss: 0.888649 \tValidation Loss: 1.333053\n",
      "Epoch: 19 \tTraining Loss: 0.872552 \tValidation Loss: 1.358982\n",
      "Epoch: 20 \tTraining Loss: 0.838957 \tValidation Loss: 1.362171\n",
      "Epoch: 21 \tTraining Loss: 0.878473 \tValidation Loss: 1.364165\n",
      "Epoch: 22 \tTraining Loss: 0.862331 \tValidation Loss: 1.373149\n",
      "Epoch: 23 \tTraining Loss: 0.876589 \tValidation Loss: 1.360949\n",
      "Epoch: 24 \tTraining Loss: 0.871401 \tValidation Loss: 1.359677\n",
      "Epoch: 25 \tTraining Loss: 0.884495 \tValidation Loss: 1.375585\n",
      "Epoch: 26 \tTraining Loss: 0.850396 \tValidation Loss: 1.378250\n",
      "Epoch: 27 \tTraining Loss: 0.852258 \tValidation Loss: 1.429852\n",
      "Epoch: 28 \tTraining Loss: 0.873932 \tValidation Loss: 1.382278\n",
      "Epoch: 29 \tTraining Loss: 0.835729 \tValidation Loss: 1.393616\n",
      "Epoch: 30 \tTraining Loss: 0.853511 \tValidation Loss: 1.392458\n",
      "Epoch: 31 \tTraining Loss: 0.858360 \tValidation Loss: 1.391495\n",
      "Epoch: 32 \tTraining Loss: 0.811100 \tValidation Loss: 1.384108\n",
      "Epoch: 33 \tTraining Loss: 0.804039 \tValidation Loss: 1.402061\n",
      "Epoch: 34 \tTraining Loss: 0.814953 \tValidation Loss: 1.423246\n",
      "Epoch: 35 \tTraining Loss: 0.831145 \tValidation Loss: 1.413798\n",
      "Epoch: 36 \tTraining Loss: 0.804145 \tValidation Loss: 1.423740\n",
      "Epoch: 37 \tTraining Loss: 0.805995 \tValidation Loss: 1.428916\n",
      "Epoch: 38 \tTraining Loss: 0.782819 \tValidation Loss: 1.418449\n",
      "Epoch: 39 \tTraining Loss: 0.837167 \tValidation Loss: 1.412276\n",
      "Epoch: 40 \tTraining Loss: 0.818203 \tValidation Loss: 1.405510\n",
      "Epoch: 41 \tTraining Loss: 0.809406 \tValidation Loss: 1.426010\n",
      "Epoch: 42 \tTraining Loss: 0.803884 \tValidation Loss: 1.426384\n",
      "Epoch: 43 \tTraining Loss: 0.780525 \tValidation Loss: 1.433062\n",
      "Epoch: 44 \tTraining Loss: 0.773353 \tValidation Loss: 1.437105\n",
      "Epoch: 45 \tTraining Loss: 0.787645 \tValidation Loss: 1.450265\n",
      "Epoch: 46 \tTraining Loss: 0.781519 \tValidation Loss: 1.466385\n",
      "Epoch: 47 \tTraining Loss: 0.759795 \tValidation Loss: 1.460995\n",
      "Epoch: 48 \tTraining Loss: 0.739118 \tValidation Loss: 1.472605\n",
      "Epoch: 49 \tTraining Loss: 0.775062 \tValidation Loss: 1.485081\n",
      "Epoch: 50 \tTraining Loss: 0.768563 \tValidation Loss: 1.473951\n"
     ]
    }
   ],
   "source": [
    "# keeping-track-of-losses \n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # keep-track-of-training-and-validation-loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # training-the-model\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # move-tensors-to-GPU \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # clear-the-gradients-of-all-optimized-variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n",
    "        output = model(data)\n",
    "        # calculate-the-batch-loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward-pass: compute-gradient-of-the-loss-wrt-model-parameters\n",
    "        loss.backward()\n",
    "        # perform-a-ingle-optimization-step (parameter-update)\n",
    "        optimizer.step()\n",
    "        # update-training-loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "    # validate-the-model\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device, dtype=torch.int64)\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # update-average-validation-loss \n",
    "        valid_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    # calculate-average-losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "        \n",
    "    # print-training/validation-statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model: 38.52813852813853 %\n"
     ]
    }
   ],
   "source": [
    "# test-the-model\n",
    "model.eval()  # it-disables-dropout\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device,  dtype=torch.int64)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "          \n",
    "    print('Test Accuracy of the model: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save \n",
    "#torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fca45b5b9d0>"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
